#!/usr/bin/env python

# ASuMa, Mar 2018
# Evaluate parses against reference parses, both in ULL format.
# See main() documentation below for usage details

import platform
import getopt
import sys
import os
import logging
import traceback

from ull.common import handle_path_string, strip_quotes, setup_logging, VERBOSITY_OPTIONS, \
    BIT_NO_LWALL, BIT_ULL_IN, BIT_NO_PERIOD, BIT_FILTER_DIR_SPEECH, BIT_STRICT_TOKENIZATION, \
    BIT_IGNORE_SENT_MISMATCH
from ull.grammar_tester import compare_ull_files, EvalError
from ull.parse_evaluator import Evaluate_Alternative


def main(argv):
    """
        Evaluates test parses against given reference parses.
        Alternatively, can generate its own sequential or random parses
        to evaluate against reference parses.
        For each sentence-parse, loops through all links in reference and 
        checks if those 2 word-instances are also connected in test parse.

        Parses must be in format:

        -----------------------------------------------------------------------
        Sentence to evaluate
        # word1 # word2
        # word2 # word3
        ...

        Another sentence to evaluate
        # word1 # word2
        ...

        -----------------------------------------------------------------------
        Usage: parse_evaluator -r <reffile> -t <testfile> [-v] [-w] [-i] [-c] [-a] [-s] [-z] [-f] [-o] [-O] [-T] [-S]

        testfile        file with parses to evaluate, or to store sequential/random parses
        reffile         file with reference parses
        -v              Verbosity level options: [none, debug, info, warning, error, critical]
        -w              File logging level options: [none, debug, info, warning, error, critical]
        -i              ignore LEFT-WALL and end-of-sentence dot, if any
        -c              ignore function words in evaluation (only take content-words, only in alternative evaluator)
        -a              use alternative parse_evaluator
        -s              generate sequential parses in <testfile> and evaluate 
        -z              generate random parses in <testfile> and evaluate 
        -f              filter sentences not matching in ref and test, or have internal dialogue
        -o              compare tokenization between two given parse files. Output file with diffs.
        -O              output directory path
        -T              strict tokenization
        -S              ignore sentences mismatch
    """
    test_file = ''
    ref_file = ''
    out_path = None
    verbose = False
    verbosity_level = logging.WARNING
    logging_level   = logging.NOTSET
    ignore_wall = False
    content = False
    sequential = False
    random = False
    alternative_parser = False
    filter_sentences = False
    compare_tokenization = False
    options = 0x00000000 | BIT_ULL_IN
    parser_type = "link-grammar-exe"

    try:
        app_name = str(os.path.split(__file__)[1]).split(".")[0]

        opts, args = getopt.getopt(argv, "ht:r:O:v:iszcafoTSw:", ["test=", "reference=", "output=", "verbosity=", "ignore",
                                                             "sequential", "random", "content", "alternative", 
                                                             "filter", "tokenization", "strict-tokenization",
                                                             "ignore-sent-mismatch", "logging"])

        for opt, arg in opts:
            if opt == '-h':
                print(main.__doc__)
                sys.exit()
            elif opt in ("-t", "--test"):
                test_file = arg
            elif opt in ("-r", "--reference"):
                ref_file = arg
            elif opt in ("-O", "--output"):
                out_path = handle_path_string(arg)
            elif opt in ("-v", "--verbosity"):
                verb_key = strip_quotes(arg)

                if verb_key not in VERBOSITY_OPTIONS.keys():
                    raise getopt.GetoptError("Wrong verbosity argument value: ()".format(verb_key))

                verbosity_level = VERBOSITY_OPTIONS[verb_key]
                verbose = verbosity_level < logging.WARNING
            elif opt in ("-w", "--logging"):
                log_key = strip_quotes(arg)

                if log_key not in VERBOSITY_OPTIONS.keys():
                    raise getopt.GetoptError("Wrong logging argument value: ()".format(log_key))

                logging_level = VERBOSITY_OPTIONS[log_key]
            elif opt in ("-i", "--ignore"):
                ignore_wall = True
                options |= (BIT_NO_LWALL | BIT_NO_PERIOD)
            elif opt in ("-c", "--content"):
                content = True
            elif opt in ("-s", "--sequential"):
                if random:
                    raise getopt.GetoptError("'--random' and '--sequential' cannot be run together.")
                sequential = True
                parser_type = "sequential"
            elif opt in ("-z", "--random"):
                if sequential:
                    raise getopt.GetoptError("'--random' and '--sequential' cannot be run together.")
                random = True
                parser_type = "random"
            elif opt in ("-a", "--alternative"):
                alternative_parser = True
            elif opt in ("-f", "--filter"):
                filter_sentences = True
                options |= BIT_FILTER_DIR_SPEECH
            elif opt in ("-o", "--tokenization"):
                compare_tokenization = True
            elif opt in ("-T", "--strict-tokenization"):
                options |= BIT_STRICT_TOKENIZATION
            elif opt in ("-S", "--ignore-sent-mismatch"):
                options |= BIT_IGNORE_SENT_MISMATCH

    except getopt.GetoptError:
        print(main.__doc__)
        sys.exit(2)

    setup_logging(verbosity_level, logging_level, f"{os.environ['PWD']}/{app_name}.log", "w")

    logger = logging.getLogger(app_name)

    logger.info("Code writen for Python3.6.4. Using: %s" % platform.python_version())

    try:
        # Check if the arguments are properly specified.
        if ref_file is None or len(ref_file) == 0:
            logger.info(main.__doc__)
            raise ValueError("Error: Arguments are not properly specified.")

        # If reference file does not exist then there is nothing to compare.
        if not os.path.isfile(ref_file) and not os.path.isdir(ref_file):
            raise FileNotFoundError(f"Error: File '{ref_file}' does not exist.")

        # Check if the arguments are properly specified.
        if test_file is None or len(test_file) == 0:
            print(main.__doc__)
            raise ValueError("Error: Incorrect test file path.")

        # Check if out_path points to an existing directory
        if out_path is not None and (not os.path.isdir(out_path)):
            raise FileNotFoundError(f"'--output' argument should point to an existing directory.")

        params = {"parser_type": parser_type}

        if out_path is not None:
            params["output_path"] = out_path

        if alternative_parser:
            Evaluate_Alternative(ref_file, test_file, verbose, ignore_wall, sequential, random, filter_sentences,
                                 compare_tokenization, content, **params)

        else:
            compare_ull_files(test_file, ref_file, options, **params)
            print(f"Evaluation complete. Results available in *.stat file(s)")


    except FileNotFoundError as err:
        logger.critical(str(err))
        return 1

    except ValueError as err:
        logger.critical(str(err))
        print(main.__doc__)
        return 1

    except EvalError as err:
        logger.critical(str(err))
        # logger.debug(traceback.print_exc())
        return 3

    except KeyboardInterrupt:
        logger.warning("Ctrl+C triggered. User abort.")
        # logger.debug(traceback.print_exc())
        return 4

    except Exception as err:
        logger.critical(str(type(err)) + ": " + str(err))
        logger.debug(traceback.print_exc())
        return 5

    return 0


if __name__ == '__main__':
    main(sys.argv[1:])
